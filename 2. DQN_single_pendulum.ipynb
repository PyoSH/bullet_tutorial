{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfe3e9f",
   "metadata": {},
   "source": [
    "# PyBullet + DQN algorithm\n",
    "\n",
    "앞서 pybullet으로 구성한 \n",
    "\n",
    "- **상태**: `[cos(theta), sin(theta), theta_dot]`\n",
    "- **행동**: 진자 관절에 가하는 토크 (-2.0 ~ 2.0)\n",
    "- **보상**: 진자가 위로 설수록, 각속도와 토크가 작을수록 높은 보상 획득."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf68805",
   "metadata": {},
   "source": [
    "## 1. Policy (DQN) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e169aa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T04:33:45.922405Z",
     "start_time": "2025-10-01T04:33:44.869467Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443aaa4",
   "metadata": {},
   "source": [
    "디버깅한 환경 클래스."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class PendulumEnv(gym.Env):\n",
    "    metadata = {\"render_modes\":[\"human\", \"rgb_array\"]}\n",
    "    \n",
    "    # record_video와 video_path 옵션을 추가하여 동영상 기록을 제어합니다.\n",
    "    def __init__(self, render_mode=\"DIRECT\", record_video=False, video_path=None):\n",
    "        super(PendulumEnv, self).__init__()\n",
    "        \n",
    "        self.render_mode = render_mode\n",
    "        self.record_video = record_video\n",
    "        self.videoPath = video_path\n",
    "\n",
    "        if p.isConnected():\n",
    "            p.disconnect()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.client = p.connect(p.GUI)\n",
    "        else:\n",
    "            self.client = p.connect(p.DIRECT)\n",
    "\n",
    "        self.g = 9.81\n",
    "        self.maxTorque = 20.0\n",
    "\n",
    "        # 비디오 녹화가 활성화된 경우에만 카메라 및 프레임 버퍼를 설정합니다.\n",
    "        if self.record_video:\n",
    "            self._setup_camera_and_video()\n",
    "        \n",
    "        # 모델 불러오기 \n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.loadURDF(\"plane.urdf\")\n",
    "        p.setGravity(0, 0, -self.g)\n",
    "\n",
    "        # **중요: 로컬 경로를 사용하도록 수정**\n",
    "        modelPath = os.path.join('models', \"pendulum_test.urdf\")\n",
    "        print(f\"model path: {modelPath}\")\n",
    "        \n",
    "        self.modelId = p.loadURDF(modelPath, useFixedBase=True)\n",
    "\n",
    "        # 사용자님의 변수명을 그대로 사용합니다.\n",
    "        self.actionSpace = spaces.Discrete(3, start=-1)\n",
    "        high = np.array([1.0, 1.0, 15.0], dtype=np.float32)\n",
    "        self.observationSpace = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "        self.stepCounter = 0\n",
    "    \n",
    "    def _setup_camera_and_video(self):\n",
    "        \"\"\"비디오 녹화에 필요한 설정을 초기화하는 헬퍼 함수\"\"\"\n",
    "        self.frames = []\n",
    "        self._render_width = 640\n",
    "        self._render_height = 480\n",
    "        \n",
    "        cam_eye_pos = [0, 2.5, 1.5]\n",
    "        cam_target_pos = [0, 0, 1]\n",
    "        cam_up_vector = [0, 0, 1]\n",
    "\n",
    "        self.view_matrix = p.computeViewMatrix(\n",
    "            cameraEyePosition=cam_eye_pos,\n",
    "            cameraTargetPosition=cam_target_pos,\n",
    "            cameraUpVector=cam_up_vector\n",
    "        )\n",
    "        self.projection_matrix = p.computeProjectionMatrixFOV(\n",
    "            fov=60.0,\n",
    "            aspect=float(self._render_width) / self._render_height,\n",
    "            nearVal=0.1,\n",
    "            farVal=100.0\n",
    "        )\n",
    "        if self.videoPath and not os.path.exists(self.videoPath):\n",
    "            os.makedirs(self.videoPath)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.stepCounter = 0\n",
    "\n",
    "        # 진자를 아래로 매달린 상태(pi)에서 약간 흔들리게 시작\n",
    "        initialAngle = np.pi + self.np_random.uniform(low=-0.1, high=0.1)\n",
    "        p.resetJointState(self.modelId, 1, targetValue=initialAngle, targetVelocity=0)\n",
    "\n",
    "        # 조인트 내부 모터 비활성화 (필수)\n",
    "        p.setJointMotorControl2(self.modelId, 1, p.VELOCITY_CONTROL, force=0)\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        torque = float(action) * self.maxTorque\n",
    "        p.setJointMotorControl2(self.modelId, 1, p.TORQUE_CONTROL, force=torque)\n",
    "        p.stepSimulation()\n",
    "\n",
    "        # **핵심 수정: record_video가 True일 때만 프레임을 캡처합니다.**\n",
    "        if self.record_video:\n",
    "            img = p.getCameraImage(self._render_width, self._render_height, \n",
    "                               viewMatrix=self.view_matrix,\n",
    "                               projectionMatrix=self.projection_matrix,\n",
    "                               renderer=p.ER_TINY_RENDERER)\n",
    "            rgb = np.array(img[2]).reshape((self._render_height, self._render_width, 4))[:,:,:3]\n",
    "            self.frames.append(rgb)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        reward, terminated = self._calculate_reward_and_done(obs, action)\n",
    "        self.stepCounter += 1\n",
    "        truncated = self.stepCounter >= 200 # 스윙업 과제는 보통 200 스텝으로 설정\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        jointState = p.getJointState(self.modelId, 1)\n",
    "        theta, theta_dot = jointState[0], jointState[1]\n",
    "        theta = (theta + np.pi) % (2*np.pi) - np.pi\n",
    "        return np.array([np.cos(theta), np.sin(theta), theta_dot], dtype=np.float32)\n",
    "    \n",
    "    def _calculate_reward_and_done(self, obs, action):\n",
    "        cos_theta, sin_theta, theta_dot = obs\n",
    "        \n",
    "        is_upright = cos_theta > 0.99\n",
    "        is_slow = abs(theta_dot) < 0.1\n",
    "        \n",
    "        terminated = bool(is_upright and is_slow)\n",
    "\n",
    "        if terminated:\n",
    "            # 성공 시, 비용이 0이 되어 가장 큰 보상을 받음\n",
    "            reward = 0\n",
    "        else:\n",
    "            # 목표: 진자를 위로 올리기 (고전적인 비용 기반 보상 함수)\n",
    "            theta = np.arctan2(sin_theta, cos_theta)\n",
    "            reward = -(theta**2 + 0.1 * abs(theta_dot) + 0.001 * (float(action)**2))\n",
    "        \n",
    "        return reward, terminated\n",
    "\n",
    "    def videoRecord(self):\n",
    "        if self.record_video and self.frames:\n",
    "            videoFile = os.path.join(self.videoPath, f\"recordings.mp4\")\n",
    "            print(f\"Video recording start... saving to {videoFile}\")\n",
    "            video = cv2.VideoWriter(videoFile, cv2.VideoWriter_fourcc(*'mp4v'), 30, (self._render_width, self._render_height))\n",
    "            # video = cv2.VideoWriter(videoFile, cv2.VideoWriter_fourcc(*'avc1'), 30, (self._render_width, self._render_height))\n",
    "            for frame in self.frames:\n",
    "                video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "            video.release()\n",
    "            print(\"Video saved.\")\n",
    "        else:\n",
    "            print(f\"Function called, but self.record_video {self.record_video} or self.frames {len(self.frames)}\")\n",
    "    \n",
    "    def close(self):        \n",
    "        if self.record_video:\n",
    "            self.videoRecord()\n",
    "        p.disconnect(self.client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd2d37",
   "metadata": {},
   "source": [
    "### DQN (Deep Q-learning Network) 이란?\n",
    "여기서부터는 공부를 하고 해보자. \n",
    "\n",
    "앞서 환경을 구성했고, 어떤 알고리즘을 사용할지 정했으니.\n",
    "\n",
    "당장 할 일이 있으니 그걸 처리하고, 이렇게 계속해서 해보자. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e26bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0412f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_space):\n",
    "        self.state_size = state_size\n",
    "        self.action_space = action_space\n",
    "        self.action_size = action_space.n\n",
    "        \n",
    "        self.action_map = {i: i + self.action_space.start for i in range(self.action_size)}\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.memory_capacity = 10000\n",
    "\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else\n",
    "            'mps' if torch.backends.mps.is_available() else\n",
    "            'cpu'\n",
    "        )\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.policy_net = QNetwork(state_size, self.action_size).to(self.device)\n",
    "        self.target_net = QNetwork(state_size, self.action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_capacity)\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        \n",
    "        action_index = np.argmax(q_values.cpu().data.numpy())\n",
    "        return self.action_map[action_index]\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        action_indices = [a - self.action_space.start for a in actions]\n",
    "        actions = torch.LongTensor(action_indices).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        current_q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c6f21",
   "metadata": {},
   "source": [
    "### 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f58d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path: models/pendulum_test.urdf\n",
      "Using device: cuda\n",
      "==================================================\n",
      "DQN 학습을 시작합니다.\n",
      "==================================================\n",
      "Episode: 1/5, Score: 1911.92, Epsilon: 0.50\n",
      "Video recording start... saving to video/train/recordings.avi\n",
      "Video saved.\n",
      "Training finished and model saved.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # videoPath를 지정하여 에피소드 종료 시 동영상이 저장되도록 합니다.\n",
    "    VIDEO_PATH = 'video/train'\n",
    "    MODEL_PATH = \"trained/dqn_pendulum_model.pth\"\n",
    "    env = PendulumEnv(render_mode=\"DIRECT\", record_video=True ,video_path=VIDEO_PATH)\n",
    "    \n",
    "    # 사용자님의 변수명을 그대로 사용합니다.\n",
    "    state_size = env.observationSpace.shape[0]\n",
    "    action_space = env.actionSpace\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_space)\n",
    "    \n",
    "    num_episodes = 5 # 주의: 100 에피소드도 매우 오래 걸릴 수 있습니다.\n",
    "    EVALUATION_STEPS = 240 * 3\n",
    "    target_update_frequency = 5\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"DQN 학습을 시작합니다.\")\n",
    "    # print(\"경고: 매 스텝마다 비디오 프레임을 캡처하므로 매우 느리게 실행됩니다.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "\n",
    "            while not (done or truncated):\n",
    "                action = agent.act(state)\n",
    "\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                agent.memory.push(state, action, reward, next_state, bool(done or truncated))\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                agent.learn()\n",
    "\n",
    "            if episode % target_update_frequency == 0:\n",
    "                agent.update_target_net()\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode: {episode+1}/{num_episodes}, Score: {-1*total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    \n",
    "    finally:\n",
    "        # 에피소드가 끝나면 env.close()를 호출하여 동영상을 저장합니다.\n",
    "        env.close()\n",
    "        \n",
    "\n",
    "    # 최종 모델 저장\n",
    "    torch.save(agent.policy_net.state_dict(), MODEL_PATH)\n",
    "    print(\"Training finished and model saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c814cba",
   "metadata": {},
   "source": [
    "### 학습된 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 4080 SUPER/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 580.65.06\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 580.65.06\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 4080 SUPER/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "model path: models/pendulum_test.urdf\n",
      "Using device: cuda\n",
      "ven = NVIDIA Corporation\n",
      "Loading trained model from 'dqn_pendulum_model.pth'...\n",
      "Starting evaluation...\n",
      "ven = NVIDIA Corporation\n",
      "Evaluation Episode 1: Total Reward = -1116.81\n",
      "Evaluation Episode 2: Total Reward = -1122.37\n",
      "Video recording start... saving to video/recordings.mp4\n",
      "Video saved.\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    \"\"\"\n",
    "    학습된 DQN 모델을 불러와 성능을 테스트하고 동영상으로 저장합니다.\n",
    "    \"\"\"\n",
    "    # --- 설정 ---\n",
    "    MODEL_PATH = \"trained/dqn_pendulum_model.pth\"\n",
    "    VIDEO_PATH = \"video\"\n",
    "    NUM_EPISODES = 2 # 테스트할 에피소드 수\n",
    "\n",
    "    # --- 모델 파일 존재 여부 확인 ---\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: 모델 파일 '{MODEL_PATH}'을 찾을 수 없습니다.\")\n",
    "        print(\"먼저 train_dqn.py를 실행하여 모델을 학습하고 저장해주세요.\")\n",
    "        return\n",
    "\n",
    "    # --- 환경 및 에이전트 설정 ---\n",
    "    # render_mode='human'으로 GUI를 띄우고, record_video=True로 동영상 저장을 활성화합니다.\n",
    "    env = PendulumEnv(render_mode=\"human\", record_video=True, video_path=VIDEO_PATH)\n",
    "    \n",
    "    state_size = env.observationSpace.shape[0]\n",
    "    action_space = env.actionSpace\n",
    "    agent = DQNAgent(state_size, action_space)\n",
    "\n",
    "    # --- 학습된 모델 로드 ---\n",
    "    print(f\"Loading trained model from '{MODEL_PATH}'...\")\n",
    "    agent.policy_net.load_state_dict(torch.load(MODEL_PATH))\n",
    "    agent.policy_net.eval() # 모델을 평가 모드로 설정 (매우 중요!)\n",
    "    \n",
    "    agent.epsilon = 0.0\n",
    "    EVALUATION_STEPS = 240 * 5 \n",
    "\n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            state, info = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "\n",
    "            for step in range(EVALUATION_STEPS):\n",
    "                # --- 최적의 행동 선택 (무작위성 없음) ---\n",
    "                # .eval() 모드에서는 신경망이 가장 높은 Q값을 갖는 행동을 선택합니다.\n",
    "                action = agent.act(state)\n",
    "                \n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                # # GUI에서 보기 좋도록 약간의 딜레이 추가\n",
    "                # time.sleep(1. / 240.)\n",
    "\n",
    "            print(f\"Evaluation Episode {episode + 1}: Total Reward = {total_reward:.2f}\")\n",
    "\n",
    "    finally:\n",
    "        # --- 환경 종료 (동영상 저장) ---\n",
    "        # env.close()가 호출되면서 동영상이 'videos' 폴더에 저장됩니다.\n",
    "        env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40218c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
